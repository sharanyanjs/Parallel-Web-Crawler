# ğŸ•·ï¸ Parallel Web Crawler (Java)

A high-performance, multi-threaded web crawler built with Java (17+), demonstrating advanced concurrency patterns, functional programming, and performance profiling.

## âœ¨ Features

### ğŸ¯ Core Functionality
- **âœ… Parallel Crawling** â€“ Multi-threaded web page processing with `ForkJoinPool`
- **âœ… Performance Profiling** â€“ Method-level execution time tracking with `@Profiled` annotations
- **âœ… JSON Configuration** â€“ Flexible configuration via JSON files
- **âœ… Word Frequency Analysis** â€“ Count and rank most frequent words
- **âœ… URL Deduplication** â€“ Thread-safe visited URL tracking

### âš¡ Technical Highlights
- **ğŸ—ï¸ Concurrency Patterns** â€“ `RecursiveAction`, `ConcurrentHashMap`, `ConcurrentSkipListSet`
- **ğŸ”„ Functional Programming** â€“ Stream API for word counting without loops
- **ğŸ­ Design Patterns** â€“ Builder, Factory, Proxy, Dependency Injection
- **ğŸ“Š Dynamic Proxies** â€“ Non-invasive method interception for profiling
- **ğŸ¯ Dependency Injection** â€“ Guice for loose coupling and testability
- **ğŸ“¦ JSON Processing** â€“ Jackson library for configuration and output

### ğŸŒŸ Unique Enhancements
- **âš¡ Smart Parallelism** â€“ Auto-scales based on available CPU cores
- **â±ï¸ Deadline Management** â€“ Respects timeout constraints gracefully
- **ğŸ“ˆ Depth Limiting** â€“ Prevents infinite crawling with configurable depth
- **ğŸ”— Link Extraction** â€“ Recursive discovery of nested pages
- **ğŸ“ Profiling Output** â€“ Detailed method execution timing reports
- **ğŸ›¡ï¸ Error Resilience** â€“ Continues crawling despite individual page failures

## ğŸ—ï¸ Architecture

ğŸ—ï¸ Architecture Overview
Layer	Component	Responsibility
Entry Point	WebCrawlerMain	Application entry point; wires dependencies and starts the crawl
Configuration	ConfigurationLoader	Loads and parses JSON configuration
Core Engine	ParallelWebCrawler	Performs parallel crawling using ForkJoinPool
Parsing	PageParserFactory	Creates page parsers for extracting links and words
Profiling	Profiler (Proxy)	Intercepts @Profiled methods and records execution times
Output	CrawlResultWriter	Writes crawl results and statistics to JSON
ğŸ” Data Flow
Step	Source	Destination	Description
1	WebCrawlerMain	ConfigurationLoader	Load crawler configuration
2	WebCrawlerMain	ParallelWebCrawler	Start crawling process
3	ParallelWebCrawler	PageParserFactory	Parse page content and links
4	ParallelWebCrawler	Profiler	Measure method execution time
5	ParallelWebCrawler	CrawlResultWriter	Persist crawl results

## ğŸš€ Quick Start

### Prerequisites
- â˜• **Java JDK 17+**
- ğŸ—ï¸ **Maven 3.6.3+**
- ğŸŒ **Internet connection** (for web crawling)
- ğŸ’» **IntelliJ IDEA** (recommended) or any Java IDE

### Installation
```bash
# 1. Clone the repository
git clone https://github.com/yourusername/parallel-web-crawler.git

# 2. Navigate to the project directory
cd parallel-web-crawler

# 3. Build the project
mvn clean package

# Run the sequential crawler (legacy implementation)
java -jar target/udacity-webcrawler-1.0.jar src/main/config/sample_config_sequential.json

# Run the parallel crawler (4 threads)
java -jar target/udacity-webcrawler-1.0.jar src/main/config/sample_config.json

ğŸ® How to Use
ğŸ‘¤ Basic Usage Flow
âš™ï¸ Configure â€“ Edit JSON configuration file

ğŸš€ Run Crawler â€“ Execute with config file

ğŸ“Š View Results â€“ Check output JSON file

ğŸ“ˆ Analyze Performance â€“ Review profiling data
