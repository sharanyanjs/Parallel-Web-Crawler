# ğŸ•·ï¸ Parallel Web Crawler (Java)

A high-performance, multi-threaded web crawler built with Java (17+), demonstrating advanced concurrency patterns, functional programming, and performance profiling.

## ğŸ“‹ Table of Contents
- [âœ¨ Features](#-features)
- [ğŸ—ï¸ Architecture](#ï¸-architecture)
- [ğŸš€ Quick Start](#-quick-start)
- [ğŸ“ Project Structure](#-project-structure)
- [ğŸ® How to Use](#-how-to-use)
- [ğŸ”§ Code Examples](#-code-examples)
- [ğŸ§ª Testing](#-testing)
- [ğŸ¤ Contributing](#-contributing)
- [ğŸ“„ License](#-license)

## âœ¨ Features

### ğŸ¯ Core Functionality
- **âœ… Parallel Crawling** â€“ Multi-threaded web page processing with `ForkJoinPool`
- **âœ… Performance Profiling** â€“ Method-level execution time tracking with `@Profiled` annotations
- **âœ… JSON Configuration** â€“ Flexible configuration via JSON files
- **âœ… Word Frequency Analysis** â€“ Count and rank most frequent words
- **âœ… URL Deduplication** â€“ Thread-safe visited URL tracking

### âš¡ Technical Highlights
- **ğŸ—ï¸ Concurrency Patterns** â€“ `RecursiveAction`, `ConcurrentHashMap`, `ConcurrentSkipListSet`
- **ğŸ”„ Functional Programming** â€“ Stream API for word counting without loops
- **ğŸ­ Design Patterns** â€“ Builder, Factory, Proxy, Dependency Injection
- **ğŸ“Š Dynamic Proxies** â€“ Non-invasive method interception for profiling
- **ğŸ¯ Dependency Injection** â€“ Guice for loose coupling and testability
- **ğŸ“¦ JSON Processing** â€“ Jackson library for configuration and output

### ğŸŒŸ Unique Enhancements
- **âš¡ Smart Parallelism** â€“ Auto-scales based on available CPU cores
- **â±ï¸ Deadline Management** â€“ Respects timeout constraints gracefully
- **ğŸ“ˆ Depth Limiting** â€“ Prevents infinite crawling with configurable depth
- **ğŸ”— Link Extraction** â€“ Recursive discovery of nested pages
- **ğŸ“ Profiling Output** â€“ Detailed method execution timing reports
- **ğŸ›¡ï¸ Error Resilience** â€“ Continues crawling despite individual page failures

## ğŸ—ï¸ Architecture

```mermaid
flowchart TD
    A[ğŸš€ WebCrawlerMain<br/>Main Entry Point] --> B[ğŸ“„ JSON Config File]
    B --> C[âš™ï¸ ConfigurationLoader]
    C --> D{ğŸ”„ Which Implementation?}
    
    D -->|Sequential| E[ğŸ¦¥ SequentialWebCrawler]
    D -->|Parallel| F[âš¡ ParallelWebCrawler]
    
    F --> G[ğŸŠâ€â™‚ï¸ ForkJoinPool]
    G --> H[ğŸ¯ CrawlTask<br/>RecursiveAction]
    
    H --> I{ğŸ“„ Parse HTML Page}
    I -->|Extract Words| J[ğŸ“Š ConcurrentHashMap<br/>Word Counts]
    I -->|Extract Links| K[ğŸ”— ConcurrentSkipListSet<br/>Visited URLs]
    
    J --> L[ğŸ“ˆ WordCounts.sort<br/>Functional Stream API]
    L --> M[ğŸ’¾ CrawlResultWriter]
    
    H --> N{â±ï¸ Timeout?}
    N -->|No| O{ğŸ“ Max Depth?}
    N -->|Yes| P[â¹ï¸ Stop Processing]
    O -->|No| Q{ğŸ”— Already Visited?}
    O -->|Yes| P
    Q -->|No| H
    Q -->|Yes| P
    
    R[ğŸ­ Profiler Module] --> S[ğŸ” ProfilingMethodInterceptor]
    S --> T[ğŸ“Š ProfilingState]
    T --> U[ğŸ“ Profile Output]
    
    E --> M
    M --> V[ğŸ‰ JSON Results]
    
    style A fill:#ff6b6b,stroke:#333,stroke-width:2px
    style F fill:#4ecdc4,stroke:#333,stroke-width:2px
    style G fill:#45b7d1,stroke:#333,stroke-width:2px
    style J fill:#96ceb4,stroke:#333,stroke-width:2px
    style K fill:#feca57,stroke:#333,stroke-width:2px
    style R fill:#ff9ff3,stroke:#333,stroke-width:2px
    style V fill:#1dd1a1,stroke:#333,stroke-width:2px
Component Architecture Diagram
graph TB
    subgraph "ğŸ“¦ Data Layer"
        D1[ConcurrentHashMap<br/>Word Counts]
        D2[ConcurrentSkipListSet<br/>Visited URLs]
        D3[ProfilingState<br/>Method Timings]
    end
    
    subgraph "ğŸ”§ Service Layer"
        S1[ParallelWebCrawler]
        S2[PageParserFactory]
        S3[WordCounts Utility]
    end
    
    subgraph "ğŸ­ Profiler Layer"
        P1[Profiler Interface]
        P2[ProfilerImpl]
        P3[ProfilingMethodInterceptor<br/>Dynamic Proxy]
    end
    
    subgraph "ğŸ“„ IO Layer"
        I1[ConfigurationLoader<br/>JSON â†’ Config]
        I2[CrawlResultWriter<br/>Results â†’ JSON]
        I3[WebCrawlerMain<br/>CLI Interface]
    end
    
    subgraph "ğŸ—ï¸ Framework Layer"
        F1[ForkJoinPool<br/>Thread Management]
        F2[RecursiveAction<br/>Task Decomposition]
        F3[Guice DI<br/>Dependency Injection]
    end
    
    I3 --> I1
    I1 --> S1
    S1 --> F1
    F1 --> F2
    F2 --> S2
    S2 --> D1
    S2 --> D2
    S1 --> S3
    S3 --> I2
    
    S1 -.-> P2
    P2 --> P3
    P3 --> D3
    D3 --> I2
    
    style D1 fill:#96ceb4
    style D2 fill:#feca57
    style D3 fill:#ff9ff3
    style S1 fill:#4ecdc4
    style P2 fill:#ff6b6b
    style F1 fill:#45b7d1
    style I1 fill:#1dd1a1
Data Flow Architecture
sequenceDiagram
    participant User as ğŸ‘¤ User
    participant Main as ğŸš€ WebCrawlerMain
    participant Config as âš™ï¸ ConfigLoader
    participant Crawler as âš¡ ParallelWebCrawler
    participant Pool as ğŸŠâ€â™‚ï¸ ForkJoinPool
    participant Task as ğŸ¯ CrawlTask
    participant Parser as ğŸ” PageParser
    participant Words as ğŸ“Š WordCounter
    participant URLs as ğŸ”— URLTracker
    participant Profiler as ğŸ“ˆ Profiler
    participant Output as ğŸ’¾ ResultWriter

    User->>Main: Execute with config.json
    Main->>Config: Load configuration
    Config-->>Main: Return config object
    
    Main->>Crawler: Start crawl(startingURLs)
    Crawler->>Pool: Create ForkJoinPool
    Crawler->>Words: Initialize ConcurrentHashMap
    Crawler->>URLs: Initialize ConcurrentSkipListSet
    
    loop For each starting URL
        Crawler->>Pool: Submit CrawlTask
        Pool->>Task: Execute compute()
        
        Task->>Task: Check depth & timeout
        Task->>URLs: Try add URL (atomic)
        URLs-->>Task: Success/Failure
        
        alt URL not visited
            Task->>Profiler: Start timing (if @Profiled)
            Task->>Parser: Parse HTML page
            Parser-->>Task: Return words & links
            Task->>Words: Merge word counts (atomic)
            
            loop For each discovered link
                Task->>Pool: Submit new CrawlTask
            end
            
            Task->>Profiler: Record duration
        end
    end
    
    Pool-->>Crawler: All tasks complete
    Crawler->>Words: Get sorted word counts
    Words-->>Crawler: Return top N words
    Crawler->>Output: Write results
    Output-->>User: Save to crawlResults.json
    
    Profiler->>Output: Write profiling data
    Output-->>User: Save to profileData.txt
Thread Concurrency Model
graph TD
    subgraph "ğŸŠâ€â™‚ï¸ ForkJoinPool (4 Threads)"
        T1[Thread 1]
        T2[Thread 2]
        T3[Thread 3]
        T4[Thread 4]
    end
    
    subgraph "ğŸ“‹ Task Queue"
        Q1[Task A: example.com]
        Q2[Task B: example.org]
        Q3[Task C: example.net]
        Q4[...]
    end
    
    subgraph "ğŸ”— Shared State"
        SH1[ConcurrentHashMap<br/>word â†’ count]
        SH2[ConcurrentSkipListSet<br/>visited URLs]
    end
    
    T1 --> Q1
    T2 --> Q2
    T3 --> Q3
    T4 --> Q4
    
    Q1 -->|Process| SA1[Subtask A1]
    Q1 -->|Process| SA2[Subtask A2]
    
    Q2 -->|Process| SB1[Subtask B1]
    Q2 -->|Process| SB2[Subtask B2]
    
    SA1 --> SH1
    SA2 --> SH1
    SB1 --> SH1
    SB2 --> SH1
    
    SA1 --> SH2
    SA2 --> SH2
    SB1 --> SH2
    SB2 --> SH2
    
    style T1 fill:#ff6b6b
    style T2 fill:#4ecdc4
    style T3 fill:#feca57
    style T4 fill:#96ceb4
    style SH1 fill:#45b7d1
    style SH2 fill:#ff9ff3
